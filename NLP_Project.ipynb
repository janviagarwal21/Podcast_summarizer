{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle practice set"
      ],
      "metadata": {
        "id": "0OGmD5Jailor"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSEValKBOnln",
        "outputId": "cf73ccb9-548f-4e76-c912-297a0807ee68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydRdyS0OrIS7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VzWJO5_v02m"
      },
      "outputs": [],
      "source": [
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE-B3KOBS3iy",
        "outputId": "71796257-da90-47e5-dcbc-60578a606594"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2023.7.22)\n",
            "Installing collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install SpeechRecognition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZAt5YBBS4jf",
        "outputId": "12f882c4-46a8-4e86-bc0e-3dfdd128ba38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text from audio: kids are talking by the door\n"
          ]
        }
      ],
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "def audio_to_text(audio_file_path):\n",
        "    # Initialize the recognizer\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    # Load the audio file\n",
        "    with sr.AudioFile(audio_file_path) as source:\n",
        "        # Adjust for ambient noise\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "\n",
        "        # Record the audio\n",
        "        audio = recognizer.record(source)\n",
        "\n",
        "    try:\n",
        "        # Recognize speech using Google Web Speech API\n",
        "        text = recognizer.recognize_google(audio)\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Google Web Speech API could not understand audio\")\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google Web Speech API; {e}\")\n",
        "\n",
        "# Specify the path to your audio file\n",
        "audio_file_path = \"/content/03-01-01-01-01-01-01.wav\"\n",
        "\n",
        "# Convert audio to text\n",
        "result = audio_to_text(audio_file_path)\n",
        "\n",
        "# Print the result\n",
        "print(\"Text from audio:\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX1Yz7hhU4xe",
        "outputId": "eed1d8f4-e3b0-45d6-fe29-6059ab524f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of /content/archiveee.zip extracted to /content/\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the path to your zip file\n",
        "zip_file_path = \"/content/archiveee.zip\"\n",
        "\n",
        "# Specify the directory where you want to extract the contents\n",
        "extracted_dir = \"/content/\"\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents to the specified directory\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "\n",
        "print(f\"Contents of {zip_file_path} extracted to {extracted_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UK5MpFdV7SA"
      },
      "source": [
        "# #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0xUkVFiYnTv"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "\n",
        "def audio_to_text(audio_file_path):\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    with sr.AudioFile(audio_file_path) as source:\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "        audio = recognizer.record(source)\n",
        "\n",
        "    try:\n",
        "        text = recognizer.recognize_google(audio)\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Google Web Speech API could not understand audio\")\n",
        "        return \"\"\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google Web Speech API; {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def transcribe_audio_from_zip(zip_file_path):\n",
        "    all_text = \"\"\n",
        "\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        extracted_dir = '/content/extracted_audio_files/'\n",
        "        zip_ref.extractall(extracted_dir)\n",
        "\n",
        "    audio_files = [file for file in os.listdir(extracted_dir) if file.endswith('.wav')]\n",
        "\n",
        "    for audio_file in audio_files:\n",
        "        audio_path = os.path.join(extracted_dir, audio_file)\n",
        "        text = audio_to_text(audio_path)\n",
        "        all_text += text + \" \"\n",
        "\n",
        "    return all_text\n",
        "\n",
        "# Specify the path to your uploaded zip file\n",
        "zip_file_path = \"/content/archiveee.zip\"\n",
        "\n",
        "# Transcribe audio from the zip file\n",
        "result_text = transcribe_audio_from_zip(zip_file_path)\n",
        "\n",
        "# Print the concatenated result text\n",
        "print(\"Combined Text from Audio Files:\", result_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn1OnRujEwXp"
      },
      "outputs": [],
      "source": [
        "RAVDESS = \"/content/extracted_audio_files\"\n",
        "ravdess_dir_lis = os.listdir(RAVDESS)\n",
        "path_list = []\n",
        "gender_list = []\n",
        "emotion_list = [\n",
        "]\n",
        "emotion_dic = {\n",
        "    '03' : 'happy',\n",
        "    '01' : 'neutral',\n",
        "    '04' : 'sad',\n",
        "    '05' : 'angry',\n",
        "    '06' : 'fear',\n",
        "    '07' : 'disgust',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuDyGBtyKjm9",
        "outputId": "a81dd697-a432-4cfa-e0a4-9125e064db30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['archive']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ravdess_dir_lis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx-mLcbSL-98"
      },
      "outputs": [],
      "source": [
        "for directory in ravdess_dir_lis:\n",
        "    actor_files = os.listdir(os.path.join(RAVDESS, directory))\n",
        "    for audio_file in actor_files:\n",
        "        part = audio_file.split('.')[0]\n",
        "        key = part.split('-')\n",
        "        if key in emotion_dic:\n",
        "            gender_code = int(part.split('-')[6])\n",
        "            path_list.append(f\"{RAVDESS}{directory}/{audio_file}\")\n",
        "            gender_list.append('female' if gender_code & 1 == 0 else 'male')\n",
        "            emotion_list.append(emotion_dic[key])\n",
        "\n",
        "ravdess_df = pd.concat([\n",
        "    pd.DataFrame(path_list, columns=['path']),\n",
        "    pd.DataFrame(gender_list, columns=['sex']),\n",
        "    pd.DataFrame(emotion_list, columns=['emotion'])\n",
        "], axis=1)\n",
        "\n",
        "ravdess_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hURFLXcAd_ch"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEWcpRmUWEwB"
      },
      "source": [
        "# Usefull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRkjPXGPd_Sb"
      },
      "outputs": [],
      "source": [
        "for dir in ravdess_dir_lis:\n",
        "  actor_files = os.listdir(os.path.join(RAVDESS, directory))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ullU_p0eJTL",
        "outputId": "ff4a03f3-01c6-4d96-e49b-49bd0047a241"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Actor_05', 'Actor_02', 'Actor_01', 'Actor_04', 'Actor_03']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actor_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBA7ZSYOrmKv"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "input_dir= Path(\"/content/extracted_audio_files/archive\")\n",
        "files = list(input_dir.rglob(\"*.wav*\"))\n",
        "files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ15SMRUu0Yl"
      },
      "outputs": [],
      "source": [
        "file =open('/content/mytxt.txt','w')\n",
        "for i in range(10):\n",
        "  #result.append(audio_to_text(str(i)))\n",
        "  text = audio_to_text(str(files[i]))\n",
        "  file.write(text + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jJLMoXZjzpaW",
        "outputId": "cde0713a-8fe8-4533-bc2a-73b8f8b48add"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'can dogs are sitting by the door'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i =0\n",
        "result = audio_to_text(str(files[14]))\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHoXvJ_VS_6V",
        "outputId": "1bcef837-af5b-4dcd-d7da-a48a28595a7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function TextIOWrapper.close()>"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file =open('/content/mytxt.txt','w')\n",
        "text = audio_to_text(str(files[0]))\n",
        "file.write(text)\n",
        "file.close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9Q9ORQLUa0C",
        "outputId": "ab3139a1-ac0e-48f3-9f94-f93c410b9d5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM2WYNWtZiXl"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rfi62zazF18z"
      },
      "source": [
        "speech to text model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO8NFXU5R0Xo",
        "outputId": "42c6ad50-fd9b-44a0-ad7d-c8c8485f91f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2023.7.22)\n",
            "Installing collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.0\n"
          ]
        }
      ],
      "source": [
        "! pip install SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yecLQeu8ZkiJ"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "def audio_to_text(audio_file_path):\n",
        "    # Initialize the recognizer\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    # Load the audio file\n",
        "    with sr.AudioFile(audio_file_path) as source:\n",
        "        # Adjust for ambient noise\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "\n",
        "        # Record the audio\n",
        "        audio = recognizer.record(source)\n",
        "\n",
        "    try:\n",
        "        # Recognize speech using Google Web Speech API\n",
        "        text = recognizer.recognize_google(audio)\n",
        "        return text\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Google Web Speech API could not understand audio\")\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results from Google Web Speech API; {e}\")\n",
        "\n",
        "# Specify the path to your audio file\n",
        "#audio_file_path = \"/content/audio_segment/segment_0-60.wav\"\n",
        "\n",
        "\n",
        "# Convert audio to text\n",
        "#result = audio_to_text(audio_file_path)\n",
        "\n",
        "# Print the result\n",
        "#print(\"Text from audio:\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou4mqSSgF4lP"
      },
      "source": [
        "zip extracter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x81pL1NCZ2lz"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the path to your zip file\n",
        "zip_file_path = \"/content/archiveee.zip\"\n",
        "\n",
        "# Specify the directory where you want to extract the contents\n",
        "extracted_dir = \"/content/\"\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents to the specified directory\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "\n",
        "print(f\"Contents of {zip_file_path} extracted to {extracted_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx3JjFHnKik0"
      },
      "source": [
        "segment podcast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpT1LKNNKs3_",
        "outputId": "3a3c2356-6480-4c81-f762-19ce452445d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmTb7zJKaBVn"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "def split_audio(input_file, output_folder, segment_duration=15):\n",
        "    # Create output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    audio = AudioSegment.from_wav(input_file)\n",
        "    audio_duration = len(audio)\n",
        "\n",
        "    for start_time in range(0, audio_duration, segment_duration * 1000):\n",
        "        end_time = min(start_time + segment_duration * 1000, audio_duration)\n",
        "        segment = audio[start_time:end_time]\n",
        "        segment_path = os.path.join(output_folder, f\"segment_{start_time//1000}-{end_time//1000}.wav\")\n",
        "        segment.export(segment_path, format=\"wav\")\n",
        "\n",
        "# Specify the path to your large audio file\n",
        "large_audio_file_path = \"/content/scotus-cotus.wav\"\n",
        "\n",
        "# Specify the output folder for segments\n",
        "output_folder = \"/content/audio_segmentsss\"\n",
        "\n",
        "# Split the audio file into segments\n",
        "split_audio(large_audio_file_path, output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNzXCA_R60d9"
      },
      "outputs": [],
      "source": [
        "#sorting the files code\n",
        "\n",
        "import os\n",
        "\n",
        "# Specify the folder containing the segmented audio files\n",
        "output_folder = \"/content/audio_segments\"\n",
        "\n",
        "# Get a list of audio files in the folder\n",
        "audio_files = os.listdir(audio_folder)\n",
        "\n",
        "# Sort the audio files based on the timestamps in their filenames\n",
        "sorted_audio_files = sorted(audio_files, key=lambda x: int(x.split('_')[1].split('-')[0]))\n",
        "\n",
        "# Iterate through the sorted audio files\n",
        "for audio_file in sorted_audio_files:\n",
        "    # Process the sorted audio files as needed\n",
        "    print(audio_file)  # Replace this with your code to use the sorted files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz2ha5jDF91v"
      },
      "source": [
        "open all files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgFeTqtmaNz0"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "input_dir= Path(\"/content/audio_segmentsss\")\n",
        "files = list(input_dir.rglob(\"*.wav*\"))\n",
        "files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfshIY1QKIE0"
      },
      "source": [
        "Text to speech for entire podcast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdTWTlsljEud"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "input_dir = Path(\"/content/audio_segment\")\n",
        "files = list(input_dir.rglob(\"*.wav*\"))\n",
        "\n",
        "textfile = open('/content/mytxt.txt', 'w')\n",
        "\n",
        "for i in range(len(files)):\n",
        "    text = audio_to_text(str(files[i]))\n",
        "\n",
        "    # Check if text is not None before writing to the file\n",
        "    if text is not None:\n",
        "        textfile.write(text + '\\n')\n",
        "    else:\n",
        "        print(f\"Failed to recognize speech in {str(files[i])}\")\n",
        "\n",
        "textfile.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOr3WCoeT9Rg",
        "outputId": "a2af638e-d7ac-4ab8-d13d-db58c826eefc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PosixPath('/content/audio_segmentsss/segment_0-15.wav'), PosixPath('/content/audio_segmentsss/segment_15-30.wav'), PosixPath('/content/audio_segmentsss/segment_30-45.wav'), PosixPath('/content/audio_segmentsss/segment_45-60.wav'), PosixPath('/content/audio_segmentsss/segment_60-75.wav'), PosixPath('/content/audio_segmentsss/segment_75-90.wav'), PosixPath('/content/audio_segmentsss/segment_90-105.wav'), PosixPath('/content/audio_segmentsss/segment_105-120.wav'), PosixPath('/content/audio_segmentsss/segment_120-135.wav'), PosixPath('/content/audio_segmentsss/segment_135-150.wav'), PosixPath('/content/audio_segmentsss/segment_150-165.wav'), PosixPath('/content/audio_segmentsss/segment_165-180.wav'), PosixPath('/content/audio_segmentsss/segment_180-195.wav'), PosixPath('/content/audio_segmentsss/segment_195-210.wav'), PosixPath('/content/audio_segmentsss/segment_210-225.wav'), PosixPath('/content/audio_segmentsss/segment_225-240.wav'), PosixPath('/content/audio_segmentsss/segment_240-255.wav'), PosixPath('/content/audio_segmentsss/segment_255-270.wav'), PosixPath('/content/audio_segmentsss/segment_270-285.wav'), PosixPath('/content/audio_segmentsss/segment_285-300.wav'), PosixPath('/content/audio_segmentsss/segment_300-315.wav'), PosixPath('/content/audio_segmentsss/segment_315-330.wav'), PosixPath('/content/audio_segmentsss/segment_330-345.wav'), PosixPath('/content/audio_segmentsss/segment_345-360.wav'), PosixPath('/content/audio_segmentsss/segment_360-375.wav'), PosixPath('/content/audio_segmentsss/segment_375-390.wav'), PosixPath('/content/audio_segmentsss/segment_390-405.wav'), PosixPath('/content/audio_segmentsss/segment_405-420.wav'), PosixPath('/content/audio_segmentsss/segment_420-435.wav'), PosixPath('/content/audio_segmentsss/segment_435-450.wav'), PosixPath('/content/audio_segmentsss/segment_450-465.wav'), PosixPath('/content/audio_segmentsss/segment_465-480.wav'), PosixPath('/content/audio_segmentsss/segment_480-495.wav'), PosixPath('/content/audio_segmentsss/segment_495-510.wav'), PosixPath('/content/audio_segmentsss/segment_510-525.wav'), PosixPath('/content/audio_segmentsss/segment_525-540.wav'), PosixPath('/content/audio_segmentsss/segment_540-555.wav'), PosixPath('/content/audio_segmentsss/segment_555-570.wav'), PosixPath('/content/audio_segmentsss/segment_570-585.wav'), PosixPath('/content/audio_segmentsss/segment_585-600.wav'), PosixPath('/content/audio_segmentsss/segment_600-615.wav'), PosixPath('/content/audio_segmentsss/segment_615-630.wav'), PosixPath('/content/audio_segmentsss/segment_630-645.wav'), PosixPath('/content/audio_segmentsss/segment_645-660.wav'), PosixPath('/content/audio_segmentsss/segment_660-675.wav'), PosixPath('/content/audio_segmentsss/segment_675-690.wav'), PosixPath('/content/audio_segmentsss/segment_690-705.wav'), PosixPath('/content/audio_segmentsss/segment_705-720.wav'), PosixPath('/content/audio_segmentsss/segment_720-735.wav'), PosixPath('/content/audio_segmentsss/segment_735-750.wav'), PosixPath('/content/audio_segmentsss/segment_750-765.wav'), PosixPath('/content/audio_segmentsss/segment_765-780.wav'), PosixPath('/content/audio_segmentsss/segment_780-795.wav'), PosixPath('/content/audio_segmentsss/segment_795-810.wav'), PosixPath('/content/audio_segmentsss/segment_810-825.wav'), PosixPath('/content/audio_segmentsss/segment_825-840.wav'), PosixPath('/content/audio_segmentsss/segment_840-855.wav'), PosixPath('/content/audio_segmentsss/segment_855-870.wav'), PosixPath('/content/audio_segmentsss/segment_870-885.wav'), PosixPath('/content/audio_segmentsss/segment_885-900.wav'), PosixPath('/content/audio_segmentsss/segment_900-915.wav'), PosixPath('/content/audio_segmentsss/segment_915-930.wav'), PosixPath('/content/audio_segmentsss/segment_930-945.wav'), PosixPath('/content/audio_segmentsss/segment_945-960.wav'), PosixPath('/content/audio_segmentsss/segment_960-975.wav'), PosixPath('/content/audio_segmentsss/segment_975-990.wav'), PosixPath('/content/audio_segmentsss/segment_990-1005.wav'), PosixPath('/content/audio_segmentsss/segment_1005-1020.wav'), PosixPath('/content/audio_segmentsss/segment_1020-1035.wav'), PosixPath('/content/audio_segmentsss/segment_1035-1050.wav'), PosixPath('/content/audio_segmentsss/segment_1050-1065.wav'), PosixPath('/content/audio_segmentsss/segment_1065-1080.wav'), PosixPath('/content/audio_segmentsss/segment_1080-1095.wav'), PosixPath('/content/audio_segmentsss/segment_1095-1110.wav'), PosixPath('/content/audio_segmentsss/segment_1110-1125.wav'), PosixPath('/content/audio_segmentsss/segment_1125-1140.wav'), PosixPath('/content/audio_segmentsss/segment_1140-1155.wav'), PosixPath('/content/audio_segmentsss/segment_1155-1170.wav'), PosixPath('/content/audio_segmentsss/segment_1170-1185.wav'), PosixPath('/content/audio_segmentsss/segment_1185-1200.wav'), PosixPath('/content/audio_segmentsss/segment_1200-1215.wav'), PosixPath('/content/audio_segmentsss/segment_1215-1230.wav'), PosixPath('/content/audio_segmentsss/segment_1230-1245.wav'), PosixPath('/content/audio_segmentsss/segment_1245-1260.wav'), PosixPath('/content/audio_segmentsss/segment_1260-1275.wav'), PosixPath('/content/audio_segmentsss/segment_1275-1290.wav'), PosixPath('/content/audio_segmentsss/segment_1290-1305.wav'), PosixPath('/content/audio_segmentsss/segment_1305-1320.wav'), PosixPath('/content/audio_segmentsss/segment_1320-1335.wav'), PosixPath('/content/audio_segmentsss/segment_1335-1350.wav'), PosixPath('/content/audio_segmentsss/segment_1350-1365.wav'), PosixPath('/content/audio_segmentsss/segment_1365-1380.wav'), PosixPath('/content/audio_segmentsss/segment_1380-1395.wav'), PosixPath('/content/audio_segmentsss/segment_1395-1410.wav'), PosixPath('/content/audio_segmentsss/segment_1410-1425.wav'), PosixPath('/content/audio_segmentsss/segment_1425-1440.wav'), PosixPath('/content/audio_segmentsss/segment_1440-1455.wav'), PosixPath('/content/audio_segmentsss/segment_1455-1470.wav'), PosixPath('/content/audio_segmentsss/segment_1470-1485.wav'), PosixPath('/content/audio_segmentsss/segment_1485-1500.wav'), PosixPath('/content/audio_segmentsss/segment_1500-1515.wav'), PosixPath('/content/audio_segmentsss/segment_1515-1530.wav'), PosixPath('/content/audio_segmentsss/segment_1530-1545.wav'), PosixPath('/content/audio_segmentsss/segment_1545-1560.wav'), PosixPath('/content/audio_segmentsss/segment_1560-1575.wav'), PosixPath('/content/audio_segmentsss/segment_1575-1590.wav'), PosixPath('/content/audio_segmentsss/segment_1590-1605.wav'), PosixPath('/content/audio_segmentsss/segment_1605-1620.wav'), PosixPath('/content/audio_segmentsss/segment_1620-1635.wav'), PosixPath('/content/audio_segmentsss/segment_1635-1650.wav'), PosixPath('/content/audio_segmentsss/segment_1650-1665.wav'), PosixPath('/content/audio_segmentsss/segment_1665-1680.wav'), PosixPath('/content/audio_segmentsss/segment_1680-1695.wav'), PosixPath('/content/audio_segmentsss/segment_1695-1710.wav'), PosixPath('/content/audio_segmentsss/segment_1710-1725.wav'), PosixPath('/content/audio_segmentsss/segment_1725-1740.wav'), PosixPath('/content/audio_segmentsss/segment_1740-1755.wav'), PosixPath('/content/audio_segmentsss/segment_1755-1770.wav'), PosixPath('/content/audio_segmentsss/segment_1770-1785.wav'), PosixPath('/content/audio_segmentsss/segment_1785-1800.wav'), PosixPath('/content/audio_segmentsss/segment_1800-1815.wav'), PosixPath('/content/audio_segmentsss/segment_1815-1830.wav'), PosixPath('/content/audio_segmentsss/segment_1830-1845.wav'), PosixPath('/content/audio_segmentsss/segment_1845-1860.wav'), PosixPath('/content/audio_segmentsss/segment_1860-1875.wav'), PosixPath('/content/audio_segmentsss/segment_1875-1890.wav'), PosixPath('/content/audio_segmentsss/segment_1890-1905.wav'), PosixPath('/content/audio_segmentsss/segment_1905-1920.wav'), PosixPath('/content/audio_segmentsss/segment_1920-1935.wav'), PosixPath('/content/audio_segmentsss/segment_1935-1950.wav'), PosixPath('/content/audio_segmentsss/segment_1950-1965.wav'), PosixPath('/content/audio_segmentsss/segment_1965-1980.wav'), PosixPath('/content/audio_segmentsss/segment_1980-1995.wav'), PosixPath('/content/audio_segmentsss/segment_1995-2010.wav'), PosixPath('/content/audio_segmentsss/segment_2010-2025.wav'), PosixPath('/content/audio_segmentsss/segment_2025-2040.wav'), PosixPath('/content/audio_segmentsss/segment_2040-2055.wav'), PosixPath('/content/audio_segmentsss/segment_2055-2070.wav'), PosixPath('/content/audio_segmentsss/segment_2070-2085.wav'), PosixPath('/content/audio_segmentsss/segment_2085-2100.wav'), PosixPath('/content/audio_segmentsss/segment_2100-2115.wav'), PosixPath('/content/audio_segmentsss/segment_2115-2130.wav'), PosixPath('/content/audio_segmentsss/segment_2130-2145.wav'), PosixPath('/content/audio_segmentsss/segment_2145-2160.wav'), PosixPath('/content/audio_segmentsss/segment_2160-2175.wav'), PosixPath('/content/audio_segmentsss/segment_2175-2190.wav'), PosixPath('/content/audio_segmentsss/segment_2190-2205.wav'), PosixPath('/content/audio_segmentsss/segment_2205-2220.wav'), PosixPath('/content/audio_segmentsss/segment_2220-2235.wav'), PosixPath('/content/audio_segmentsss/segment_2235-2250.wav'), PosixPath('/content/audio_segmentsss/segment_2250-2265.wav'), PosixPath('/content/audio_segmentsss/segment_2265-2280.wav'), PosixPath('/content/audio_segmentsss/segment_2280-2295.wav'), PosixPath('/content/audio_segmentsss/segment_2295-2310.wav'), PosixPath('/content/audio_segmentsss/segment_2310-2325.wav'), PosixPath('/content/audio_segmentsss/segment_2325-2340.wav'), PosixPath('/content/audio_segmentsss/segment_2340-2355.wav'), PosixPath('/content/audio_segmentsss/segment_2355-2370.wav'), PosixPath('/content/audio_segmentsss/segment_2370-2385.wav'), PosixPath('/content/audio_segmentsss/segment_2385-2400.wav'), PosixPath('/content/audio_segmentsss/segment_2400-2415.wav'), PosixPath('/content/audio_segmentsss/segment_2415-2430.wav'), PosixPath('/content/audio_segmentsss/segment_2430-2445.wav'), PosixPath('/content/audio_segmentsss/segment_2445-2460.wav'), PosixPath('/content/audio_segmentsss/segment_2460-2475.wav'), PosixPath('/content/audio_segmentsss/segment_2475-2490.wav'), PosixPath('/content/audio_segmentsss/segment_2490-2505.wav'), PosixPath('/content/audio_segmentsss/segment_2505-2520.wav'), PosixPath('/content/audio_segmentsss/segment_2520-2535.wav'), PosixPath('/content/audio_segmentsss/segment_2535-2550.wav'), PosixPath('/content/audio_segmentsss/segment_2550-2565.wav'), PosixPath('/content/audio_segmentsss/segment_2565-2580.wav'), PosixPath('/content/audio_segmentsss/segment_2580-2595.wav'), PosixPath('/content/audio_segmentsss/segment_2595-2610.wav'), PosixPath('/content/audio_segmentsss/segment_2610-2625.wav'), PosixPath('/content/audio_segmentsss/segment_2625-2640.wav'), PosixPath('/content/audio_segmentsss/segment_2640-2655.wav'), PosixPath('/content/audio_segmentsss/segment_2655-2670.wav'), PosixPath('/content/audio_segmentsss/segment_2670-2685.wav'), PosixPath('/content/audio_segmentsss/segment_2685-2700.wav'), PosixPath('/content/audio_segmentsss/segment_2700-2715.wav'), PosixPath('/content/audio_segmentsss/segment_2715-2730.wav'), PosixPath('/content/audio_segmentsss/segment_2730-2745.wav'), PosixPath('/content/audio_segmentsss/segment_2745-2760.wav'), PosixPath('/content/audio_segmentsss/segment_2760-2775.wav'), PosixPath('/content/audio_segmentsss/segment_2775-2790.wav'), PosixPath('/content/audio_segmentsss/segment_2790-2805.wav'), PosixPath('/content/audio_segmentsss/segment_2805-2820.wav'), PosixPath('/content/audio_segmentsss/segment_2820-2835.wav'), PosixPath('/content/audio_segmentsss/segment_2835-2850.wav'), PosixPath('/content/audio_segmentsss/segment_2850-2861.wav')]\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "file_paths = [\n",
        "    Path('/content/audio_segments/segment_0-15.wav'),\n",
        "    Path('/content/audio_segments/segment_120-135.wav'),\n",
        "    Path('/content/audio_segments/segment_30-45.wav'),\n",
        "    Path('/content/audio_segments/segment_135-143.wav'),\n",
        "    Path('/content/audio_segments/segment_90-105.wav'),\n",
        "    Path('/content/audio_segments/segment_45-60.wav'),\n",
        "    Path('/content/audio_segments/segment_75-90.wav'),\n",
        "    Path('/content/audio_segments/segment_60-75.wav'),\n",
        "    Path('/content/audio_segments/segment_105-120.wav'),\n",
        "    Path('/content/audio_segments/segment_15-30.wav')\n",
        "]\n",
        "\n",
        "# Extract timestamps from filenames\n",
        "timestamps = [int(str(path.stem).split('_')[-1].split('-')[0]) for path in files]\n",
        "\n",
        "# Combine file paths and timestamps, then sort based on timestamps\n",
        "sorted_files = sorted(zip(files, timestamps), key=lambda x: x[1])\n",
        "\n",
        "# Extract only file paths from the sorted list\n",
        "sorted_file_paths = [path for path, timestamp in sorted_files]\n",
        "\n",
        "print(sorted_file_paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FjVmKw95nnwJ"
      },
      "outputs": [],
      "source": [
        "textfile = open('/content/Start-.txt', 'w')\n",
        "\n",
        "for i in range(len(sorted_file_paths)):\n",
        "    text = audio_to_text(str(sorted_file_paths[i]))\n",
        "\n",
        "    # Check if text is not None before writing to the file\n",
        "    if text is not None:\n",
        "        textfile.write(text)# + '\\n')\n",
        "    else:\n",
        "        print(f\"Failed to recognize speech in {str(sorted_file_paths[i])}\")\n",
        "\n",
        "textfile.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUMMARIZER"
      ],
      "metadata": {
        "id": "mQV9dj4YfU74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the text file into a DataFrame\n",
        "txt_file_path = \"/content/Start- (1).txt\"  # Replace with the path to your text file\n",
        "with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
        "    text_content = file.read()\n",
        "\n",
        "# Create a DataFrame with a single column 'article content'\n",
        "df = pd.DataFrame({'article content': [text_content]})\n",
        "\n",
        "# Create a summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "# Function to generate a summary for each article\n",
        "def generate_summary(article):\n",
        "    # Split the article into chunks of 1000 tokens\n",
        "    max_chunk_length = 100\n",
        "    chunks = [article[i:i + max_chunk_length] for i in range(0, len(article), max_chunk_length)]\n",
        "\n",
        "    # Generate summaries for each chunk\n",
        "    summaries = [summarizer(chunk, max_length=100, min_length=10, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
        "\n",
        "    # Combine the summaries of the chunks\n",
        "    return \" \".join(summaries)\n",
        "\n",
        "# Apply the summarization function to the single article in the DataFrame\n",
        "df['Summary'] = df['article content'].apply(generate_summary)\n",
        "\n",
        "# Set pandas options to display the entire content of the 'Summary' column\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Print or save the DataFrame with the summary\n",
        "print(df[['Summary']])\n",
        "\n",
        "# Save the DataFrame with the summary to a new TXT file\n",
        "df[['Summary']].to_csv(\"/content/Start- (1).txt\", index=False, header=False, sep='\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ATWqnX4fUKe",
        "outputId": "9a014b77-9a49-4c1a-cdad-7a56578fda59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 100, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
            "Your max_length is set to 100, but your input_length is only 26. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
            "Your max_length is set to 100, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
            "Your max_length is set to 100, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
            "Your max_length is set to 100, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
            "Your max_length is set to 100, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
            "Your max_length is set to 100, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
            "Your max_length is set to 100, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
            "Your max_length is set to 100, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 22. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
            "Your max_length is set to 100, but your input_length is only 18. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n",
            "Your max_length is set to 100, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
            "Your max_length is set to 100, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
            "Your max_length is set to 100, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
            "Your max_length is set to 100, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
            "Your max_length is set to 100, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
            "Your max_length is set to 100, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
            "Your max_length is set to 100, but your input_length is only 24. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n",
            "Your max_length is set to 100, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
            "Your max_length is set to 100, but your input_length is only 10. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Summary\n",
            "0   This week will be discussing developments in the New York attorney general's fraud case . This week's episode will focus on the fraud case involving the attorney general . The case is being investigated by New York's Department of Justice .  A gag order being put on hold for now by a New York appellate judge . Farm Fresh pre-proportioned products are being pre-cooked by Farm Fresh . The gag order was lifted by an appeals court judge .  ngredients and seasonal recipes delivered right to your doorstep . Use code sisters free for free bakers .  One breakfast item per box while subscription is active . Look for the link to hel hel .  lofresh America's number one meal kit in our show notes this week .  Attorney General James moved to Washington, D.C. for the first time . America's top meal kit is America's #1 meal kit .  The experts were irrelevant because of fraud was a fraud, the president said . He wanted to exclude certain experts that he wanted to call . He said fraud was the only reason to call certain experts .  lready decided was correct in allowing them to testify . There's still the question as to the amount .  The engorgement of the discouragement that should take place increase value in havin.  of the damages . of the . damages the engorgment of the  discouragement . that should . take place, it should not be the result of discouragement. of the damage .  A New York appellate judge put it on hold during the trial on Thursday .  The judges law Greenfield after they falsely accused her of being Senator Schumer's girlfriend . Greenfield was falsely accused of being the senator's girlfriend and accused of having an affair with Schumer .  The judge said it was n.  complained about her sitting next to an advising judge angering the judge . The judge agreed that it was not the first time she had sat next to a advising judge .  Donald Trump has filed a separate lawsu to prevent Trump senior and his attorneys from suing Trump senior . The lawsuit was drafted to prevent the president and his lawyers from suing senior Trump . Trump senior has already filed a lawsuit against the president .  it against judge Ingram to preclude him from gagging Trump . The judge so far who looked at it preli. it against him preli .  Trump is in a position that's narrowly tailor tailor tailor-made for him . The president has been under fire for his comments on the White House . The White House says the gag order should be suspended .  Donald Trump this week filed a motion to dismiss the e. ed . ed to achieve that interest is appropriate .  Judge anger on ruled against Donald Trump's latest motion a motion . Trump has been accused of fraudulently defrauding millions of dollars in the past .  Blue Land is having its best sale of the year . Jill says she sa. for dismissal of the entire case .  Jill says he wants to distance himself from the failure to impose a protective order . Jill says it is not about the failure of the order to impose the order .  \"Kind of makes the case stronger because you have participants co-defendants who are saying yeah we d. kind of makes it stronger,\" she says .  She says defendants have no real incentive to prevent this from happening . id all these bad behaviors . id .  Federalist Society member clerked on the Georgia Supreme Court for a republican Supreme Court .  Barbara Barbara is a former colleague of our state of namias from the the Northern Dis. urt Justice who is Barbara Barbara . Barbara Barbara was Barbara Barbara's former colleague .  Judge Willis has now moved to revoke the bond for Harrison Floyd h. trict of Georgia .  Limits on speech are narrowly tailored to achieve a compelling governmental interest . But that limits on speech must be narrowly tailored for a compelling government interest . That doesn't mean that speech is narrowly tailored narrowly to achieve an important government goal, it means that it's not narrowly tailored .  An ordinary citizen is an ordinary citizen . The motion was passed in the House of Representatives on Tuesday night .  Barb Barb says she is grateful that Fanny Willis was able to get a restraining order against Floyd against Floyd . Barb Barb: \"I'm grateful that Willis is able to do so much for me\" Barb Barb is proud that Willis won't be able to sue Floyd Floyd .  Barb: It' would mean that we could have a decision before the election . Barb: Barb: 'It' would be a good time for us to get a decision' Barb: \"It would be great for Barb to be able to make a decision in time for the election\"  It's good to have a date on the books so that you're not before the November 2024 election so that that vote can be done before November 2024 .  Barb Barb: Barb Barb is the most dangerous case in the country . Barb Barb says she hopes voters will be able to make an informed decision in the U.S.  For Trump it's clear he mishandled very sensitive National secrets . Lomi is the biggest innovatio in the U.S. history .  Loomies new app lets you track your environmental activity . The app has been in use in the modern-day kitchen since the dishwasher .  Earn points for every cycle and redeem for freebies . Earn points every cycle to earn points and redeem a freebie . Use the points to redeem freebies and earn points for each cycle .  Their own ethics code was unveiled this week . The U.S. House of Representatives introduced its ethics code to the public this year .\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0OGmD5Jailor",
        "_UK5MpFdV7SA",
        "LEWcpRmUWEwB"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}